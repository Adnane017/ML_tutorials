{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in semi-structured text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the raw text\n",
    "rawData = open(\"./Datasets/SMSSpamCollection.tsv\").read()\n",
    "\n",
    "# Print the raw data\n",
    "rawData[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first replace the \"\\t\" characters with \"\\n\" then split based on \"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData = rawData.replace('\\t', '\\n').split('\\n')\n",
    "parsedData[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a list of all labels and a list of all text entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Check the first 5 labels:\n",
      "['ham', 'spam', 'ham', 'ham', 'ham']\n",
      "\n",
      "- Check the first 5 text entries:\n",
      "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\", \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", \"Nah I don't think he goes to usf, he lives around here though\", 'Even my brother is not like to speak with me. They treat me like aids patent.', 'I HAVE A DATE ON SUNDAY WITH WILL!!']\n",
      "\n",
      "- Check the length of the labels: 5571\n",
      "\n",
      "- Check the length of the text enteries: 5570\n"
     ]
    }
   ],
   "source": [
    "labelList = parsedData[0::2]\n",
    "textList = parsedData[1::2]\n",
    "# Check the data\n",
    "print(\"- Check the first 5 labels:\")\n",
    "print(labelList[0:5])\n",
    "print(\"\\n- Check the first 5 text entries:\")\n",
    "print(textList[0:5])\n",
    "# Check the dimensions\n",
    "print(\"\\n- Check the length of the labels: {}\".format(len(labelList)))\n",
    "print(\"\\n- Check the length of the text enteries: {}\".format(len(textList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_list\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fullCorpus = pd.DataFrame({\n",
    "    'label': labelList[:-1],\n",
    "    'body_list': textList\n",
    "})\n",
    "\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to import the data directly from the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep=\"\\t\", header=None)\n",
    "fullCorpus.columns = ['label', 'body_text']\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data has 5568 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions\n",
    "print(\"The input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5568 rows, 746 are spam, 4822 are ham\n"
     ]
    }
   ],
   "source": [
    "# How many spam/ham are there?\n",
    "print(\"Out of {} rows, {} are spam, {} are ham\".format(len(fullCorpus),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='spam']),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='ham'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing text data\n",
    "\n",
    "Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n",
    "1. **Remove punctuation**\n",
    "2. **Tokenization**\n",
    "3. **Remove stopwords**\n",
    "4. **Lemmatize/Stem**\n",
    "\n",
    "Note that lemmatizing and stemming are helpful but not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Max width of columns when dispalying datasets\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# Import dataset and set column names\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "2                                          Nah I dont think he goes to usf he lives around here though  \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function that remove punctuation from a single sentence\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
    "# Display the resulting dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a function that produces tokens based on a single sentence\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x))\n",
    "# Display the resulting dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Number of stop words in English: 179\n",
      "['i', \"you've\", 'himself', 'they', 'that', 'been', 'a', 'while', 'through', 'in', 'here', 'few', 'own', 'just', 're', 'doesn', 'ma', \"shouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# List of stopwords in English\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(\"- Number of stop words in English: {}\".format(len(stopword)))\n",
    "print(stopword[:len(stopword):10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function that remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "# Display the resulting dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation and stopwords removal and tokenization can all be done using one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preliminary steps\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Function that perform punctuation and stopwords removal and tokenization\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "# Display the resulting dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Stemming (Porter stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MARTIN_EXTENSIONS', 'NLTK_EXTENSIONS', 'ORIGINAL_ALGORITHM', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_abc_impl', '_apply_rule_list', '_contains_vowel', '_ends_cvc', '_ends_double_consonant', '_has_positive_measure', '_is_consonant', '_measure', '_replace_suffix', '_step1a', '_step1b', '_step1c', '_step2', '_step3', '_step4', '_step5a', '_step5b', 'mode', 'pool', 'stem', 'unicode_repr', 'vowels']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "# List of methods of the PorterStemmer\n",
    "print(dir(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROW stem:\n",
      "- grows becomes grow\n",
      "- growing becomes grow\n",
      "- grow becomes grow\n",
      "RUN stem:\n",
      "- run becomes run\n",
      "- running becomes run\n",
      "- runner becomes runner\n"
     ]
    }
   ],
   "source": [
    "# Test the Porter stemmer\n",
    "# GROW stem\n",
    "print(\"GROW stem:\")\n",
    "print(\"- grows becomes {}\".format(ps.stem('grows')))\n",
    "print(\"- growing becomes {}\".format(ps.stem('growing')))\n",
    "print(\"- grow becomes {}\".format(ps.stem('grow')))\n",
    "# RUN stem\n",
    "print(\"RUN stem:\")\n",
    "print(\"- run becomes {}\".format(ps.stem('run')))\n",
    "print(\"- running becomes {}\".format(ps.stem('running')))\n",
    "print(\"- runner becomes {}\".format(ps.stem('runner')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "4                                                                                       [date, sunday]   \n",
       "\n",
       "                                                                                     body_text_stemmed  \n",
       "0  [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "2                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "3                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function that transform words into their stems\n",
    "def steming(tokenized_text):\n",
    "    stem_list = [ps.stem(word) for word in tokenized_text]\n",
    "    return stem_list\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: steming(x))\n",
    "# Disply the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Lemmatizer ([WordNet](https://wordnet.princeton.edu/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', 'lemmatize', 'unicode_repr']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Lemmatizer and Stemmer\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# List of methods of WordNet lemmatizer\n",
    "print(dir(wn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer for goose/geese:\n",
      "- goose becomes goos\n",
      "- geese becomes gees\n",
      "Lemmatizer for goose/geese:\n",
      "- goose becomes goose\n",
      "- geese becomes goose\n"
     ]
    }
   ],
   "source": [
    "# Difference between WordNetLemmatizer and PorterStemmer\n",
    "# Stemmer\n",
    "print(\"Stemmer for goose/geese:\")\n",
    "print(\"- goose becomes {}\".format(ps.stem('goose')))\n",
    "print(\"- geese becomes {}\".format(ps.stem('geese')))\n",
    "# Lemmatizer\n",
    "print(\"Lemmatizer for goose/geese:\")\n",
    "print(\"- goose becomes {}\".format(wn.lemmatize('goose')))\n",
    "print(\"- geese becomes {}\".format(wn.lemmatize('geese')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "4                                                                                       [date, sunday]   \n",
       "\n",
       "                                                                                     body_text_stemmed  \\\n",
       "0  [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...   \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "2                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "3                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "4                                                                                       [date, sunday]   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \n",
       "0  [ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "3                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function that transform words into their lemma\n",
    "def lemmatizing(tokenized_text):\n",
    "    lemma_list = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return lemma_list\n",
    "\n",
    "# Apply it to the whole dataset\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "# Disply the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "\n",
    "Creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary steps\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "ps = nltk.PorterStemmer()\n",
    "# List of stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# Load data\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the document-term matrix is: (5567, 8104)\n",
      "- The first 20 column names:\n",
      "['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# Create the document-term matrix based on counts\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "# Display the dimension of the resulting matrix\n",
    "print(f'The dimension of the document-term matrix is: {X_counts.shape}')\n",
    "# Display the first 20 column names (terms) \n",
    "print(\"- The first 20 column names:\")\n",
    "print(count_vect.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output sparse matrices\n",
    "\n",
    "_A sparse matrix is a matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5567x8104 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50122 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information we get when we type the name of the document-term matrix\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  008704050406  0089mi  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0  0  0             0       0     0            0            0           0   0   \n",
       "1  0  0             0       0     0            0            0           0   0   \n",
       "2  0  0             0       0     0            0            0           0   0   \n",
       "3  0  0             0       0     0            0            0           0   0   \n",
       "4  0  0             0       0     0            0            0           0   0   \n",
       "\n",
       "   020603 ...   zindgi  zoe  zogtoriu  zoom  zouk  zyada  é  ü  üll  〨ud  \n",
       "0       0 ...        0    0         0     0     0      0  0  0    0    0  \n",
       "1       0 ...        0    0         0     0     0      0  0  0    0    0  \n",
       "2       0 ...        0    0         0     0     0      0  0  0    0    0  \n",
       "3       0 ...        0    0         0     0     0      0  0  0    0    0  \n",
       "4       0 ...        0    0         0     0     0      0  0  0    0    0  \n",
       "\n",
       "[5 rows x 8104 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some enteries of the matrix\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "# Add column names\n",
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "# Display the matrix\n",
    "X_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Vectorization\n",
    "\n",
    "Creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n",
    "\n",
    "\"NLP is an interesting topic\"\n",
    "\n",
    "| n | Name      | Tokens                                                         |\n",
    "|---|-----------|----------------------------------------------------------------|\n",
    "| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n",
    "| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n",
    "| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary steps\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "ps = nltk.PorterStemmer()\n",
    "# List of stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# Load data\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...  \n",
       "1                                                            nah dont think goe usf live around though  \n",
       "2                                                        even brother like speak treat like aid patent  \n",
       "3                                                                                          date sunday  \n",
       "4  per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "# Apply the function to clean the text\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply CountVectorizer based on n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The dimension of the document-term matrix is: (5567, 31260)\n",
      "- The first 20 column names:\n",
      "['008704050406 sp', '0089mi last', '0121 2025050', '01223585236 xx', '01223585334 cum', '0125698789 ring', '02 user', '020603 2nd', '0207 153', '02072069400 bx', '02073162414 cost', '02085076972 repli', '020903 2nd', '021 3680', '021 3680offer', '050703 tcsbcm4235wc1n3xx', '06 good', '07046744435 arrang', '07090298926 reschedul', '07099833605 reschedul']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "# Create the document-term matrix based on counts\n",
    "X_counts = ngram_vect.fit_transform(data['cleaned_text'])\n",
    "# Display the dimension of the resulting matrix\n",
    "print(f'- The dimension of the document-term matrix is: {X_counts.shape}')\n",
    "# Display the first 20 column names (terms) \n",
    "print(\"- The first 20 column names:\")\n",
    "print(ngram_vect.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5567x31260 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 43714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information we get when we type the name of the document-term matrix\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406 sp</th>\n",
       "      <th>0089mi last</th>\n",
       "      <th>0121 2025050</th>\n",
       "      <th>01223585236 xx</th>\n",
       "      <th>01223585334 cum</th>\n",
       "      <th>0125698789 ring</th>\n",
       "      <th>02 user</th>\n",
       "      <th>020603 2nd</th>\n",
       "      <th>0207 153</th>\n",
       "      <th>02072069400 bx</th>\n",
       "      <th>...</th>\n",
       "      <th>zoe 18</th>\n",
       "      <th>zoe hit</th>\n",
       "      <th>zogtoriu stare</th>\n",
       "      <th>zoom cine</th>\n",
       "      <th>zouk nichol</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>üll finish</th>\n",
       "      <th>üll submit</th>\n",
       "      <th>üll take</th>\n",
       "      <th>〨ud even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   008704050406 sp  0089mi last  0121 2025050  01223585236 xx  \\\n",
       "0                0            0             0               0   \n",
       "1                0            0             0               0   \n",
       "2                0            0             0               0   \n",
       "3                0            0             0               0   \n",
       "4                0            0             0               0   \n",
       "\n",
       "   01223585334 cum  0125698789 ring  02 user  020603 2nd  0207 153  \\\n",
       "0                0                0        0           0         0   \n",
       "1                0                0        0           0         0   \n",
       "2                0                0        0           0         0   \n",
       "3                0                0        0           0         0   \n",
       "4                0                0        0           0         0   \n",
       "\n",
       "   02072069400 bx    ...     zoe 18  zoe hit  zogtoriu stare  zoom cine  \\\n",
       "0               0    ...          0        0               0          0   \n",
       "1               0    ...          0        0               0          0   \n",
       "2               0    ...          0        0               0          0   \n",
       "3               0    ...          0        0               0          0   \n",
       "4               0    ...          0        0               0          0   \n",
       "\n",
       "   zouk nichol  zyada kisi  üll finish  üll submit  üll take  〨ud even  \n",
       "0            0           0           0           0         0         0  \n",
       "1            0           0           0           0         0         0  \n",
       "2            0           0           0           0         0         0  \n",
       "3            0           0           0           0         0         0  \n",
       "4            0           0           0           0         0         0  \n",
       "\n",
       "[5 rows x 31260 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some enteries of the matrix\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "# Add column names\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "# Display the matrix\n",
    "X_counts_df.head()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term frequency-inverse document frequency)\n",
    "\n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Here $i$ refers to terms (words) and $j$ refers to rows (sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Apply the function to clean the text\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The dimension of the document-term matrix is: (5567, 8104)\n",
      "- The first 20 column names:\n",
      "['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "# Create the document-term matrix based on counts\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "# Display the dimension of the resulting matrix\n",
    "print(f'- The dimension of the document-term matrix is: {X_tfidf.shape}')\n",
    "# Display the first 20 column names (terms) \n",
    "print(\"- The first 20 column names:\")\n",
    "print(tfidf_vect.get_feature_names()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5567x8104 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50122 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information we get when we type the name of the document-term matrix\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  008704050406  0089mi  0121  01223585236  01223585334  0125698789  \\\n",
       "0  0.0  0.0           0.0     0.0   0.0          0.0          0.0         0.0   \n",
       "1  0.0  0.0           0.0     0.0   0.0          0.0          0.0         0.0   \n",
       "2  0.0  0.0           0.0     0.0   0.0          0.0          0.0         0.0   \n",
       "3  0.0  0.0           0.0     0.0   0.0          0.0          0.0         0.0   \n",
       "4  0.0  0.0           0.0     0.0   0.0          0.0          0.0         0.0   \n",
       "\n",
       "    02  020603 ...   zindgi  zoe  zogtoriu  zoom  zouk  zyada    é    ü  üll  \\\n",
       "0  0.0     0.0 ...      0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "1  0.0     0.0 ...      0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "2  0.0     0.0 ...      0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "3  0.0     0.0 ...      0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "4  0.0     0.0 ...      0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "\n",
       "   〨ud  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 8104 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some enteries of the matrix\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "# Add column names\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()\n",
    "# Display the matrix\n",
    "X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Feature Creation\n",
    "\n",
    "Here we add some features like text length and the percentage of punctuation to have a better chance to predict well the response variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\".\\Datasets\\SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature for text message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>punct%</th>\n",
       "      <th>body_length</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "   text_length  punct%  body_length  body_len  \n",
       "0          128     4.7          128       128  \n",
       "1           49     4.1           49        49  \n",
       "2           62     3.2           62        62  \n",
       "3           28     7.1           28        28  \n",
       "4          135     4.4          135       135  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature for percentage of text that is punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "   text_length  punct%  \n",
       "0          128     4.7  \n",
       "1           49     4.1  \n",
       "2           62     3.2  \n",
       "3           28     7.1  \n",
       "4          135     4.4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function that compute the percentage of puctuation in a string\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "# Apply to our dataset\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "# Display first rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms comparing the length of spam and ham messages\n",
    "bins = np.linspace(0, 200, 40)\n",
    "pyplot.hist(data[data['label']=='spam']['body_len'], bins, alpha=0.5, normed=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['body_len'], bins, alpha=0.5, normed=True, label='ham')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms comparing the puctuation percentages of spam and ham messages\n",
    "bins = np.linspace(0, 50, 40)\n",
    "pyplot.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, normed=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, normed=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Transformations\n",
    "\n",
    "#### Box-Cox Power Transformation\n",
    "\n",
    "| X    | Base Form           |           Transformation               |\n",
    "|------|--------------------------|--------------------------|\n",
    "| -2   | $$ y ^ {-2} $$           | $$ \\frac{1}{y^2} $$      |\n",
    "| -1   | $$ y ^ {-1} $$           | $$ \\frac{1}{y} $$        |\n",
    "| -0.5 | $$ y ^ {\\frac{-1}{2}} $$ | $$ \\frac{1}{\\sqrt{y}} $$ |\n",
    "| 0    | $$ y^{0} $$              | $$ log(y) $$             |\n",
    "| 0.5  | $$ y ^ {\\frac{1}{2}}  $$ | $$ \\sqrt{y} $$           |\n",
    "| 1    | $$ y^{1} $$              | $$ y $$                  |\n",
    "| 2    | $$ y^{2} $$              | $$ y^2 $$                |\n",
    "\n",
    "\n",
    "**Process**\n",
    "1. Determine what range of exponents to test\n",
    "2. Apply each transformation to each value of your chosen feature\n",
    "3. Use some criteria to determine which of the transformations yield the best distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the two features but this time considering all types of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFn9JREFUeJzt3XuU5GV95/H3JyAaBRmBweAMOriye2Szq3JmlV01a8QYAXXIRgzGyMSQsNmjWT3qhlETNWeNQmJiYi7mEFGBeIF4Y7wkgYNi1s1iGBBERGUkg4wzmRm5CeIN/O4f9XRSNN3T1TPdVd3PvF/n9KlfPb+nf/XtX1V/6qnn96uqVBWSpH792KQLkCQtLoNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr32WpI1SSrJ/pOuZU8luTzJry7wNu9O8tgF2tbrkryrLS/o/k7y6FbrfguxPS09Bv0+KsmWJN9t/+C3J/lkkiMnUMeCB+wIt/mmJH+1F7//jCQ/avvu7iRbk1yU5D8N96uqA6vqphG2tXWu26yqt1TVguyndt8/a2jb32i13rcQ29fSY9Dv255XVQcCRwA7gD+ZcD3Lyba27w4CjgO+AvyfJMcv9A0t51dKWhoMelFV3wM+BBwz1Zbk4CTnJ9mV5OYkv5Xkx9q6/ZK8Lcm3ktwEnDT0e6ckuWp4+0leneRj860ryXFJ/iHJHUmuTfKMoXWXJ/nfSf5vkruSXJLksKH1p7W6b03y21Oj2CTPAV4H/EIbjV87dJOPmW17u9l3VVVbq+oNwLuAs4dqqCSPa8snJvly2/Y3k7wmycOAvwEeNfTq4FHtFceHkvxVkm8DvzzLq5BfSbItyfYkrx663fcmefPQ9X951ZDkAuDRwMfb7f3m9KmgVsPGJLcl2Zzk14a29ab26uX89rdcn2TtXPtJk2XQiyQPBX4BuGKo+U+Ag4HHAv8VOA14aVv3a8BzgScBa4EXDP3eRuCoJI8favsl4IJ51rQK+CTwZuAQ4DXAh5OsHOr2i62mw4EDWh+SHAP8OfBiBq9WDgZWAVTV3wJvAS5s0xVPmGt78/AR4NgW4NOdC/z3qjoI+Eng01X1HeAE2quD9rOt9V/H4Ml3BfC+WW7vp4GjgWcDG4anY2ZTVS8BvkF7NVdVvzdDtw8AW4FHMbhv3zLtlcrzgQ+22jYCfzrX7WqyDPp928eS3AF8G/gZ4PdhMGJnEPyvraq7qmoL8AfAS9rvvRD4o6q6papuA946tcGq+j5wIYNwJ8m/B9YAn5hnbb8EfKqqPlVVP6qqS4FNwIlDfd5TVV+rqu8CFwFPbO0vAD5eVZ+rqh8AbwBG+VCn2bY3qm1AGATgdD8Ejkny8Kq6vaqunmNb/6+qPtb+9u/O0ud3quo7VXUd8B7gRfOs9wHacZqnAWdW1feq6hoGr1ReMtTtc+1+uY/BE/gTZtiUlhCDft92clWtAB4MvBz4bJKfAA5jMKK9eajvzbRRMYOR3i3T1g07D/jFJGEQEBe1J4D5eAxwSpu2uaM9IT2NwQh9yj8PLd8DHDhTfVV1D3DrCLc52/ZGtYrBE8odM6z7eQZPUjcn+WyS/zzHtm6ZY/30Pjcz+Lv31qOA26rqrmnbXjV0ffp+eojHEZY2g15U1X1V9RHgPgZh+i0GI9DHDHV7NPDNtrwdOHLauuHtXQH8AHg6g+mQeU3bNLcAF1TViqGfh1XVWSP87nZg9dSVJD8OHDpc4h7UM4qfA65uUzL3U1VXVtU6BtNCH2PwimF3tYxS4/T7YGra5zvAQ4fW/cQ8tr0NOCTJQdO2/c1Z+msZMOhFBtYBjwBuaC/JLwJ+N8lBSR4DvAqYOhh4EfA/k6xO8ghgwwybPZ/B3O29VfW5OUrYP8lDhn4e1G7reUl+th38fUg7qLh6jm3BYG77eUn+S5IDgN9hMKUyZQewZurg8t5o+25VkjcCv8rgQO/0PgckeXGSg6vqhwymyqZOZdwBHJrk4D24+d9O8tA2PfZSBlNmANcAJyY5pL1Ce+W039vB4NjLA1TVLcA/AG9t+/w/Aqcz+3ECLQMG/b7t40nuZhA8vwusr6rr27rfYDAyvAn4HPB+4N1t3V8CfwdcC1zN4CDkdBcwOOg4ymj+ncB3h37e0wJnHYPg3MVghP+/GOEx2/6G32BwwHA7cBewE5iaPvrrdnlrkrnmymfzqLbv7gauBP4D8IyqumSW/i8BtrSzaH6ddgyjqr7C4ODnTW2Kaj7TL58FNgOXAW8buu0LGNw3W4BL+NcngClvBX6r3d5MB5xfxOC4yjbgo8Ab2zESLVPxi0e0GNp0yU7g2Kq6ccK1HMhg3vzoqvqnSdYiTYIjei2W/wFcOamQT/K8Nq3xMOBtwHUMRrjSPscj5VpwSbYwmBM/eYJlrGMwhREGp2WeWr581T7KqRtJ6pxTN5LUuSUxdXPYYYfVmjVrJl2GJC0rV1111beqauVc/ZZE0K9Zs4ZNmzZNugxJWlaSTH9X+oycupGkzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1bqSgT7IlyXVJrkmyqbUdkuTSJDe2y0e09iR5R/v2+C8mOXYx/wBJ0u7N552xP11V3xq6vgG4rKrOSrKhXT+TwbfaH91+nsLgSyWeskD1LjlrNnxy1nVbzjppjJVI0sz2ZupmHYMvgaZdnjzUfn4NXAGsSHLETBuQJC2+UYO+gEuSXJXkjNb2yKraDtAuD2/tq7j/t9Nv5f7fIA9AkjOSbEqyadeuXXtWvSRpTqNO3Ty1qrYlORy4NMlXdtM3M7Q94EPvq+oc4ByAtWvX+qH4krRIRhrRV9W2drmTwZcFPxnYMTUl0y53tu5bgSOHfn01gy8ZliRNwJxBn+RhSQ6aWgaeDXwJ2Aisb93WAxe35Y3Aae3sm+OAO6emeCRJ4zfK1M0jgY8mmer//qr62yRXAhclOR34BnBK6/8p4ERgM3AP8NIFr1qSNLI5g76qbgKeMEP7rcDxM7QX8LIFqU6StNd8Z6wkdc6gl6TOGfSS1Lkl8eXgvdrdxyOAH5EgaTwc0UtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md8ztjlzC/c1bSQjDoJ2iuIJekheDUjSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnRs56JPsl+QLST7Rrh+V5PNJbkxyYZIDWvuD2/XNbf2axSldkjSK+YzoXwHcMHT9bODtVXU0cDtwems/Hbi9qh4HvL31kyRNyEhBn2Q1cBLwrnY9wDOBD7Uu5wEnt+V17Tpt/fGtvyRpAkYd0f8R8JvAj9r1Q4E7quredn0rsKotrwJuAWjr72z97yfJGUk2Jdm0a9euPSxfkjSXOYM+yXOBnVV11XDzDF1rhHX/2lB1TlWtraq1K1euHKlYSdL8jfKhZk8Fnp/kROAhwMMZjPBXJNm/jdpXA9ta/63AkcDWJPsDBwO3LXjlkqSRzDmir6rXVtXqqloDnAp8uqpeDHwGeEHrth64uC1vbNdp6z9dVQ8Y0UuSxmNvzqM/E3hVks0M5uDPbe3nAoe29lcBG/auREnS3pjX59FX1eXA5W35JuDJM/T5HnDKAtQmSVoAvjNWkjpn0EtS5wx6Seqc3xk7B7/XVdJy54hekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnZsz6JM8JMk/Jrk2yfVJfqe1H5Xk80luTHJhkgNa+4Pb9c1t/ZrF/RMkSbszyoj++8Azq+oJwBOB5yQ5DjgbeHtVHQ3cDpze+p8O3F5VjwPe3vpJkiZkzqCvgbvb1Qe1nwKeCXyotZ8HnNyW17XrtPXHJ8mCVSxJmpeR5uiT7JfkGmAncCnwdeCOqrq3ddkKrGrLq4BbANr6O4FDZ9jmGUk2Jdm0a9euvfsrJEmzGinoq+q+qnoisBp4MvD4mbq1y5lG7/WAhqpzqmptVa1duXLlqPVKkuZpXmfdVNUdwOXAccCKJPu3VauBbW15K3AkQFt/MHDbQhQrSZq/Uc66WZlkRVv+ceBZwA3AZ4AXtG7rgYvb8sZ2nbb+01X1gBG9JGk89p+7C0cA5yXZj8ETw0VV9YkkXwY+mOTNwBeAc1v/c4ELkmxmMJI/dRHqliSNaM6gr6ovAk+aof0mBvP109u/B5yyINVJkvaa74yVpM4Z9JLUuVHm6LVErdnwyd2u33LWSWOqRNJS5ohekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM7tP+kCJC2MNRs+udv1W846aUyVaKlxRC9JnXNE37HdjfAc3S1Pc43apZk4opekzjmi14yc75X64Yhekjo3Z9AnOTLJZ5LckOT6JK9o7YckuTTJje3yEa09Sd6RZHOSLyY5drH/CEnS7EYZ0d8LvLqqHg8cB7wsyTHABuCyqjoauKxdBzgBOLr9nAG8c8GrliSNbM45+qraDmxvy3cluQFYBawDntG6nQdcDpzZ2s+vqgKuSLIiyRFtO1oiPHtD2nfMa44+yRrgScDngUdOhXe7PLx1WwXcMvRrW1vb9G2dkWRTkk27du2af+WSpJGMHPRJDgQ+DLyyqr69u64ztNUDGqrOqaq1VbV25cqVo5YhSZqnkU6vTPIgBiH/vqr6SGveMTUlk+QIYGdr3wocOfTrq4FtC1Ww1DOn1LQYRjnrJsC5wA1V9YdDqzYC69vyeuDiofbT2tk3xwF3Oj8vSZMzyoj+qcBLgOuSXNPaXgecBVyU5HTgG8Apbd2ngBOBzcA9wEsXtGJJ0ryMctbN55h53h3g+Bn6F/CyvaxLkrRAfGesJHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0md8xumtEf8Bipp+XBEL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnfOdsVoUvnNWWjoMemmM5noClBaDUzeS1DmDXpI6Z9BLUucMeknqnAdjNRG7Oyi5nM/I8WCrliJH9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalznl6pJccPRFsc7td9l0Gv7hho0v3NGfRJ3g08F9hZVT/Z2g4BLgTWAFuAF1bV7UkC/DFwInAP8MtVdfXilC6Nn2+I0nI0yoj+vcCfAucPtW0ALquqs5JsaNfPBE4Ajm4/TwHe2S6lJcOw1r5mzoOxVfX3wG3TmtcB57Xl84CTh9rPr4ErgBVJjlioYiVJ87enZ908sqq2A7TLw1v7KuCWoX5bW9sDJDkjyaYkm3bt2rWHZUiS5rLQB2MzQ1vN1LGqzgHOAVi7du2MfaSZOPUizc+ejuh3TE3JtMudrX0rcORQv9XAtj0vT5K0t/Y06DcC69vyeuDiofbTMnAccOfUFI8kaTJGOb3yA8AzgMOSbAXeCJwFXJTkdOAbwCmt+6cYnFq5mcHplS9dhJolSfMwZ9BX1YtmWXX8DH0LeNneFiVJWjh+1o0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpc/v8N0z5AVmSeueIXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekznV/Hr3nyUva1zmil6TOGfSS1DmDXpI6Z9BLUucMeknqXPdn3UhaGHOdwbblrJPGVInma9kHvadPSgvD/6V+OXUjSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOrfsT6+UtDTs7vTMuc6x9xz9xeWIXpI6tygj+iTPAf4Y2A94V1WdtRi3I2l52Ns3Y+3tiH9vXm3MZTm8GlnwoE+yH/BnwM8AW4Erk2ysqi8v9G1JEizuu3on/SS1EBZjRP9kYHNV3QSQ5IPAOsCgl7Tk7Asf/bAYQb8KuGXo+lbgKdM7JTkDOKNdvTvJV/fw9g4DvrWHv7uYrGt+rGv+lmpt1jUPOXuv6nrMKJ0WI+gzQ1s9oKHqHOCcvb6xZFNVrd3b7Sw065of65q/pVqbdc3POOpajLNutgJHDl1fDWxbhNuRJI1gMYL+SuDoJEclOQA4Fdi4CLcjSRrBgk/dVNW9SV4O/B2D0yvfXVXXL/TtDNnr6Z9FYl3zY13zt1Rrs675WfS6UvWA6XNJUkd8Z6wkdc6gl6TOLeugT/KcJF9NsjnJhgnWcWSSzyS5Icn1SV7R2t+U5JtJrmk/J06gti1Jrmu3v6m1HZLk0iQ3tstHjLmmfze0T65J8u0kr5zE/kry7iQ7k3xpqG3G/ZOBd7TH2xeTHDvmun4/yVfabX80yYrWvibJd4f221+Mua5Z77ckr23766tJfnbMdV04VNOWJNe09nHur9myYbyPsapalj8MDvR+HXgscABwLXDMhGo5Aji2LR8EfA04BngT8JoJ76ctwGHT2n4P2NCWNwBnT/h+/GcGb/wY+/4Cfgo4FvjSXPsHOBH4GwbvFTkO+PyY63o2sH9bPnuorjXD/Sawv2a839r/wLXAg4Gj2v/rfuOqa9r6PwDeMIH9NVs2jPUxtpxH9P/yUQtV9QNg6qMWxq6qtlfV1W35LuAGBu8QXqrWAee15fOAkydYy/HA16vq5knceFX9PXDbtObZ9s864PwauAJYkeSIcdVVVZdU1b3t6hUM3qMyVrPsr9msAz5YVd+vqn8CNjP4vx1rXUkCvBD4wGLc9u7sJhvG+hhbzkE/00ctTDxck6wBngR8vjW9vL0Ee/e4p0iaAi5JclUGHzsB8Miq2g6DByJw+ATqmnIq9/8HnPT+gtn3z1J6zP0Kg5HflKOSfCHJZ5M8fQL1zHS/LZX99XRgR1XdONQ29v01LRvG+hhbzkE/0kctjFOSA4EPA6+sqm8D7wT+DfBEYDuDl4/j9tSqOhY4AXhZkp+aQA0zyuANdc8H/ro1LYX9tTtL4jGX5PXAvcD7WtN24NFV9STgVcD7kzx8jCXNdr8tif0FvIj7DybGvr9myIZZu87Qttf7bDkH/ZL6qIUkD2JwR76vqj4CUFU7quq+qvoR8Jcs0svW3amqbe1yJ/DRVsOOqZeD7XLnuOtqTgCurqodrcaJ769mtv0z8cdckvXAc4EXV5vUbVMjt7blqxjMhf/bcdW0m/ttKeyv/YH/Blw41Tbu/TVTNjDmx9hyDvol81ELbQ7wXOCGqvrDofbhubWfA740/XcXua6HJTloapnBwbwvMdhP61u39cDF46xryP1GWpPeX0Nm2z8bgdPamRHHAXdOvfwehwy+0OdM4PlVdc9Q+8oMvgeCJI8FjgZuGmNds91vG4FTkzw4yVGtrn8cV13Ns4CvVNXWqYZx7q/ZsoFxP8bGceR5sX4YHKH+GoNn5NdPsI6nMXh59UXgmvZzInABcF1r3wgcMea6HsvgrIdrgeun9hFwKHAZcGO7PGQC++yhwK3AwUNtY99fDJ5otgM/ZDCaOn22/cPgZfWftcfbdcDaMde1mcH87dRj7C9a359v9++1wNXA88Zc16z3G/D6tr++Cpwwzrpa+3uBX5/Wd5z7a7ZsGOtjzI9AkKTOLeepG0nSCAx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1Ln/D8QXz94HtGPuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms depicting the length of messages\n",
    "bins = np.linspace(0, 200, 40)\n",
    "pyplot.hist(data['body_len'], bins)\n",
    "pyplot.title(\"Body Length Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms depicting the the puctuation percentage\n",
    "bins = np.linspace(0, 50, 40)\n",
    "pyplot.hist(data['punct%'], bins)\n",
    "pyplot.title(\"Punctuation % Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the power transformation to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    pyplot.hist(data['punct%']**(1/i), bins = 40)\n",
    "    pyplot.title(\"Transformation: 1/{}\".format(str(i)))\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic Random Forest model\n",
    "\n",
    "Here we will explore the basic steps to follow in order to build a random forest model. Later we will discuss how to perform cross-validation to select good hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%     0  008704050406  0089mi  0121  01223585236  \\\n",
       "0       128     4.7  0  0             0       0     0            0   \n",
       "1        49     4.1  0  0             0       0     0            0   \n",
       "2        62     3.2  0  0             0       0     0            0   \n",
       "3        28     7.1  0  0             0       0     0            0   \n",
       "4       135     4.4  0  0             0       0     0            0   \n",
       "\n",
       "   01223585334  0125698789 ...   zindgi  zoe  zogtoriu  zoom  zouk  zyada  é  \\\n",
       "0            0           0 ...        0    0         0     0     0      0  0   \n",
       "1            0           0 ...        0    0         0     0     0      0  0   \n",
       "2            0           0 ...        0    0         0     0     0      0  0   \n",
       "3            0           0 ...        0    0         0     0     0      0  0   \n",
       "4            0           0 ...        0    0         0     0     0      0  0   \n",
       "\n",
       "   ü  üll  〨ud  \n",
       "0  0    0    0  \n",
       "1  0    0    0  \n",
       "2  0    0    0  \n",
       "3  0    0    0  \n",
       "4  0    0    0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "# List of stop words + initialise the Porter stemmer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# Add text length and punct percentage features\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# Create the document-term matrix based on counts\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "\n",
    "#tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "#X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# Display some enteries of the matrix\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "# Add column names\n",
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "\n",
    "# Construct the feature matrix by adding the text length and punct percentage features to the count features\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], X_counts_df], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_make_estimator', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# List of methods available with the random forest classifier\n",
    "print(dir(RandomForestClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important useful methods are 'feature_importances_', 'fit' and 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Display the parameters requires to run the RandomForestClassifier()\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would usually play with the parameters max_depth (max depth of each tree) and n_estimator (number of trees). Use n_jobs=-1 to run the model in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore RandomForestClassifier through Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96947935, 0.96947935, 0.9703504 , 0.96046721, 0.96585804])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RF with 5 fold-CV\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore RandomForestClassifier through Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and test sets. The same order has to always be respected\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random forest model in parallel with 50 trees and a max depth of 20. \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.06078235369516432, 'body_len'),\n",
       " (0.03874973871451262, 'mobil'),\n",
       " (0.03376640454233298, 'call'),\n",
       " (0.03000027947536334, 'claim'),\n",
       " (0.029951654279227316, 'txt'),\n",
       " (0.025132708469617876, 'stop'),\n",
       " (0.02399443933589941, 'text'),\n",
       " (0.023145369093081847, 'prize'),\n",
       " (0.017228589987067933, 'free'),\n",
       " (0.016521259649633795, 'servic')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.593 / Accuracy: 0.941\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from the test features\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# Compute accuracy, precision and recall\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3),\n",
    "                                                        round((y_pred==y_test).sum() / len(y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings by building our own Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and test sets. The same order has to always be respected\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that compute precision, recall and accuracy for certain values of n_est and depth, then print the results \n",
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.235 / Accuracy: 0.907\n",
      "Est: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.529 / Accuracy: 0.943\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.632 / Accuracy: 0.955\n",
      "Est: 10 / Depth: None ---- Precision: 1.0 / Recall: 0.728 / Accuracy: 0.967\n",
      "Est: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.235 / Accuracy: 0.907\n",
      "Est: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.537 / Accuracy: 0.943\n",
      "Est: 50 / Depth: 30 ---- Precision: 1.0 / Recall: 0.632 / Accuracy: 0.955\n",
      "Est: 50 / Depth: None ---- Precision: 1.0 / Recall: 0.772 / Accuracy: 0.972\n",
      "Est: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.206 / Accuracy: 0.903\n",
      "Est: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.544 / Accuracy: 0.944\n",
      "Est: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.647 / Accuracy: 0.957\n",
      "Est: 100 / Depth: None ---- Precision: 1.0 / Recall: 0.765 / Accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "# Compute perf measures for a set of 12 possible param combination\n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46.480312</td>\n",
       "      <td>1.717683</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973235</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.023886</td>\n",
       "      <td>2.976516</td>\n",
       "      <td>0.446022</td>\n",
       "      <td>0.100949</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973056</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.800745</td>\n",
       "      <td>0.248674</td>\n",
       "      <td>0.171541</td>\n",
       "      <td>0.051096</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.976640</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972696</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997978</td>\n",
       "      <td>0.997081</td>\n",
       "      <td>0.998653</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.996408</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>0.000845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.380753</td>\n",
       "      <td>0.617691</td>\n",
       "      <td>0.354813</td>\n",
       "      <td>0.070821</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972337</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.998653</td>\n",
       "      <td>0.998922</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.043070</td>\n",
       "      <td>1.111407</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.107369</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.974888</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.974843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972157</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "8       46.480312      1.717683         0.493458        0.132369   \n",
       "11      44.023886      2.976516         0.446022        0.100949   \n",
       "6        2.800745      0.248674         0.171541        0.051096   \n",
       "7       23.380753      0.617691         0.354813        0.070821   \n",
       "10      24.043070      1.111407         0.449541        0.107369   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                300   \n",
       "11            None                300   \n",
       "6               90                 10   \n",
       "7               90                150   \n",
       "10            None                150   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.978475   \n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.978475   \n",
       "6      {'max_depth': 90, 'n_estimators': 10}           0.978475   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.976682   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.974888   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "8            0.973944           0.973944       ...                0.973235   \n",
       "11           0.973944           0.973944       ...                0.973056   \n",
       "6            0.976640           0.972147       ...                0.972696   \n",
       "7            0.972147           0.973944       ...                0.972337   \n",
       "10           0.973944           0.974843       ...                0.972157   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "8         0.003198                1            0.999102            0.999102   \n",
       "11        0.003561                2            1.000000            1.000000   \n",
       "6         0.004361                3            0.997978            0.997081   \n",
       "7         0.003577                4            0.999102            0.998877   \n",
       "10        0.003416                5            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "8             0.998877            0.998877            0.998877   \n",
       "11            1.000000            1.000000            1.000000   \n",
       "6             0.998653            0.998428            0.996408   \n",
       "7             0.998877            0.999102            0.998653   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "8           0.998967         0.000110  \n",
       "11          1.000000         0.000000  \n",
       "6           0.997710         0.000845  \n",
       "7           0.998922         0.000168  \n",
       "10          1.000000         0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a grid seach with CV using GridSearchCV from sklearn\n",
    "\n",
    "# Initialise the random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "# Parameters to explore\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "# Perform the grid search\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_features, data['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic Gradient boosting model\n",
    "\n",
    "Here we will explore the basic steps to follow in order to build a gradient boosting model. Later we will discuss how to perform cross-validation to select good hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%     0  008704050406  0089mi  0121  01223585236  \\\n",
       "0       128     4.7  0  0             0       0     0            0   \n",
       "1        49     4.1  0  0             0       0     0            0   \n",
       "2        62     3.2  0  0             0       0     0            0   \n",
       "3        28     7.1  0  0             0       0     0            0   \n",
       "4       135     4.4  0  0             0       0     0            0   \n",
       "\n",
       "   01223585334  0125698789 ...   zindgi  zoe  zogtoriu  zoom  zouk  zyada  é  \\\n",
       "0            0           0 ...        0    0         0     0     0      0  0   \n",
       "1            0           0 ...        0    0         0     0     0      0  0   \n",
       "2            0           0 ...        0    0         0     0     0      0  0   \n",
       "3            0           0 ...        0    0         0     0     0      0  0   \n",
       "4            0           0 ...        0    0         0     0     0      0  0   \n",
       "\n",
       "   ü  üll  〨ud  \n",
       "0  0    0    0  \n",
       "1  0    0    0  \n",
       "2  0    0    0  \n",
       "3  0    0    0  \n",
       "4  0    0    0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "# List of stop words + initialise the Porter stemmer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# Add text length and punct percentage features\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# Create the document-term matrix based on counts\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "\n",
    "# Display some enteries of the matrix\n",
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "# Add column names\n",
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "\n",
    "# Construct the feature matrix by adding the text length and punct percentage features to the count features\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], X_counts_df], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore GradientBoostingClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_SUPPORTED_LOSS', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_initialized', '_check_params', '_clear_state', '_decision_function', '_estimator_type', '_fit_stage', '_fit_stages', '_get_param_names', '_init_decision_function', '_init_state', '_is_initialized', '_make_estimator', '_resize_state', '_staged_decision_function', '_validate_estimator', '_validate_y', 'apply', 'decision_function', 'feature_importances_', 'fit', 'get_params', 'n_features', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'staged_decision_function', 'staged_predict', 'staged_predict_proba']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# List of methods available with the random forest classifier\n",
    "print(dir(GradientBoostingClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important useful methods are 'feature_importances_', 'fit' and 'predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Display the parameters requires to run the GradientBoostingClassifier()\n",
    "print(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would usually play with the parameters learning_rate, max_depth (max depth of each tree) and n_estimator (number of trees). Gradient boosting cannot be parallelised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore GradientBoostingClassifier through Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\model_selection\\_split.py:18: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96140036, 0.97307002, 0.96226415, 0.95597484, 0.9541779 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RF with 5 fold-CV\n",
    "gb = GradientBoostingClassifier()\n",
    "k_fold = KFold(n_splits=5)\n",
    "cross_val_score(gb, X_features, data['label'], cv=k_fold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore GradientBoostingClassifier through Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and test sets. The same order has to always be respected\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a GBM with a learning rate of 0.1, 100 trees and a max depth of 10. \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(learning_rate = 0.1, n_estimators=100, max_depth=10)\n",
    "gb_model = gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.100107347481215, 'body_len'),\n",
       " (0.03823685305955173, 'call'),\n",
       " (0.028422968464892923, 'txt'),\n",
       " (0.015269302465387354, 'user'),\n",
       " (0.01450590916417374, 'text'),\n",
       " (0.014032134369035856, '88066'),\n",
       " (0.012703969197119995, 'free'),\n",
       " (0.012685110772297028, 'httpalto18coukwavewaveaspo44345'),\n",
       " (0.012559907312523792, 'punct%'),\n",
       " (0.012275034392939397, 'bid')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(gb_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.928 / Recall: 0.823 / Accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from the test features\n",
    "y_pred = gb_model.predict(X_test)\n",
    "# Compute accuracy, precision and recall\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3),\n",
    "                                                        round((y_pred==y_test).sum() / len(y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings by building our own Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and test sets. Carful with the order\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that compute precision, recall and accuracy for certain values of n_est and depth, then print the results \n",
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djedamski/.pyenv/versions/3.5.3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 50 / Depth: 3 / LR: 0.01 ---- Precision: 0.0 / Recall: 0.0 / Accuracy: 0.868\n",
      "Est: 50 / Depth: 3 / LR: 0.1 ---- Precision: 1.0 / Recall: 0.687 / Accuracy: 0.959\n",
      "Est: 50 / Depth: 3 / LR: 1 ---- Precision: 0.88 / Recall: 0.796 / Accuracy: 0.959\n",
      "Est: 50 / Depth: 7 / LR: 0.01 ---- Precision: 0.0 / Recall: 0.0 / Accuracy: 0.868\n",
      "Est: 50 / Depth: 7 / LR: 0.1 ---- Precision: 0.968 / Recall: 0.83 / Accuracy: 0.974\n",
      "Est: 50 / Depth: 7 / LR: 1 ---- Precision: 0.917 / Recall: 0.823 / Accuracy: 0.967\n",
      "Est: 50 / Depth: 11 / LR: 0.01 ---- Precision: 1.0 / Recall: 0.027 / Accuracy: 0.872\n",
      "Est: 50 / Depth: 11 / LR: 0.1 ---- Precision: 0.962 / Recall: 0.871 / Accuracy: 0.978\n",
      "Est: 50 / Depth: 11 / LR: 1 ---- Precision: 0.926 / Recall: 0.85 / Accuracy: 0.971\n",
      "Est: 50 / Depth: 15 / LR: 0.01 ---- Precision: 0.0 / Recall: 0.0 / Accuracy: 0.868\n",
      "Est: 50 / Depth: 15 / LR: 0.1 ---- Precision: 0.977 / Recall: 0.857 / Accuracy: 0.978\n",
      "Est: 50 / Depth: 15 / LR: 1 ---- Precision: 0.919 / Recall: 0.85 / Accuracy: 0.97\n",
      "Est: 100 / Depth: 3 / LR: 0.01 ---- Precision: 0.987 / Recall: 0.51 / Accuracy: 0.934\n",
      "Est: 100 / Depth: 3 / LR: 0.1 ---- Precision: 0.991 / Recall: 0.776 / Accuracy: 0.969\n",
      "Est: 100 / Depth: 3 / LR: 1 ---- Precision: 0.901 / Recall: 0.803 / Accuracy: 0.962\n",
      "Est: 100 / Depth: 7 / LR: 0.01 ---- Precision: 0.989 / Recall: 0.612 / Accuracy: 0.948\n",
      "Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.985 / Recall: 0.871 / Accuracy: 0.981\n",
      "Est: 100 / Depth: 7 / LR: 1 ---- Precision: 0.922 / Recall: 0.81 / Accuracy: 0.966\n",
      "Est: 100 / Depth: 11 / LR: 0.01 ---- Precision: 0.991 / Recall: 0.741 / Accuracy: 0.965\n",
      "Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.984 / Recall: 0.864 / Accuracy: 0.98\n",
      "Est: 100 / Depth: 11 / LR: 1 ---- Precision: 0.912 / Recall: 0.844 / Accuracy: 0.969\n",
      "Est: 100 / Depth: 15 / LR: 0.01 ---- Precision: 0.992 / Recall: 0.796 / Accuracy: 0.972\n",
      "Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.977 / Recall: 0.871 / Accuracy: 0.98\n",
      "Est: 100 / Depth: 15 / LR: 1 ---- Precision: 0.932 / Recall: 0.844 / Accuracy: 0.971\n",
      "Est: 150 / Depth: 3 / LR: 0.01 ---- Precision: 0.988 / Recall: 0.537 / Accuracy: 0.938\n",
      "Est: 150 / Depth: 3 / LR: 0.1 ---- Precision: 0.992 / Recall: 0.81 / Accuracy: 0.974\n",
      "Est: 150 / Depth: 3 / LR: 1 ---- Precision: 0.902 / Recall: 0.816 / Accuracy: 0.964\n",
      "Est: 150 / Depth: 7 / LR: 0.01 ---- Precision: 0.99 / Recall: 0.687 / Accuracy: 0.958\n",
      "Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.977 / Recall: 0.857 / Accuracy: 0.978\n",
      "Est: 150 / Depth: 7 / LR: 1 ---- Precision: 0.937 / Recall: 0.81 / Accuracy: 0.968\n",
      "Est: 150 / Depth: 11 / LR: 0.01 ---- Precision: 0.983 / Recall: 0.796 / Accuracy: 0.971\n",
      "Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.985 / Recall: 0.871 / Accuracy: 0.981\n",
      "Est: 150 / Depth: 11 / LR: 1 ---- Precision: 0.904 / Recall: 0.837 / Accuracy: 0.967\n",
      "Est: 150 / Depth: 15 / LR: 0.01 ---- Precision: 0.975 / Recall: 0.796 / Accuracy: 0.97\n",
      "Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.977 / Recall: 0.864 / Accuracy: 0.979\n",
      "Est: 150 / Depth: 15 / LR: 1 ---- Precision: 0.913 / Recall: 0.857 / Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>368.558762</td>\n",
       "      <td>0.308644</td>\n",
       "      <td>0.969643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'n_estimators': 150, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.128055</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214.185205</td>\n",
       "      <td>0.303001</td>\n",
       "      <td>0.969283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'n_estimators': 150, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.241176</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>381.475534</td>\n",
       "      <td>0.206334</td>\n",
       "      <td>0.968744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'n_estimators': 150, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.965022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.316129</td>\n",
       "      <td>0.036329</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157.876316</td>\n",
       "      <td>0.351555</td>\n",
       "      <td>0.968205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.965919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.158135</td>\n",
       "      <td>0.187909</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>353.960683</td>\n",
       "      <td>0.258787</td>\n",
       "      <td>0.968205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.964126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.086312</td>\n",
       "      <td>0.056744</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "5     368.558762         0.308644         0.969643               1.0   \n",
       "2     214.185205         0.303001         0.969283               1.0   \n",
       "8     381.475534         0.206334         0.968744               1.0   \n",
       "1     157.876316         0.351555         0.968205               1.0   \n",
       "7     353.960683         0.258787         0.968205               1.0   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "5                 0.1              11                150   \n",
       "2                 0.1               7                150   \n",
       "8                 0.1              15                150   \n",
       "1                 0.1               7                100   \n",
       "7                 0.1              15                100   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "5  {'n_estimators': 150, 'learning_rate': 0.1, 'm...                1   \n",
       "2  {'n_estimators': 150, 'learning_rate': 0.1, 'm...                2   \n",
       "8  {'n_estimators': 150, 'learning_rate': 0.1, 'm...                3   \n",
       "1  {'n_estimators': 100, 'learning_rate': 0.1, 'm...                4   \n",
       "7  {'n_estimators': 100, 'learning_rate': 0.1, 'm...                4   \n",
       "\n",
       "   split0_test_score       ...         split2_test_score  split2_train_score  \\\n",
       "5           0.965919       ...                  0.966757                 1.0   \n",
       "2           0.965919       ...                  0.968553                 1.0   \n",
       "8           0.965022       ...                  0.969452                 1.0   \n",
       "1           0.965919       ...                  0.968553                 1.0   \n",
       "7           0.964126       ...                  0.969452                 1.0   \n",
       "\n",
       "   split3_test_score  split3_train_score  split4_test_score  \\\n",
       "5           0.968553                 1.0           0.969452   \n",
       "2           0.964960                 1.0           0.967655   \n",
       "8           0.965858                 1.0           0.967655   \n",
       "1           0.964960                 1.0           0.965858   \n",
       "7           0.966757                 1.0           0.967655   \n",
       "\n",
       "   split4_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "5                 1.0      7.128055        0.030754        0.004141   \n",
       "2                 1.0      2.241176        0.043978        0.005181   \n",
       "8                 1.0     44.316129        0.036329        0.003816   \n",
       "1                 1.0      3.158135        0.187909        0.003954   \n",
       "7                 1.0     10.086312        0.056744        0.002968   \n",
       "\n",
       "   std_train_score  \n",
       "5              0.0  \n",
       "2              0.0  \n",
       "8              0.0  \n",
       "1              0.0  \n",
       "7              0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining parameter values to input to the GridSearchCV() function \n",
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "# Run a Grid search\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_features, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct way to run RF/GBM\n",
    "\n",
    "Actually, the right way to train the models with train/test sets is to split the data first, then fit the count vectorizer. Previously, we did things in the inverse order. so next we will perform parameter tuning based on RF using the correct approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\model_selection\\_split.py:18: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# List of stop words + initialise the Porter stemmer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv(\"./Datasets/SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "# Add text length and punct percentage features\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide into training and test sets. Careful with the order\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>...</th>\n",
       "      <th>zed</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%     0  008704050406  0089mi  0121  01223585236  02  \\\n",
       "0        20     5.0  0  0             0       0     0            0   0   \n",
       "1       109     5.5  0  0             0       0     0            0   0   \n",
       "2        71     2.8  0  0             0       0     0            0   0   \n",
       "3       138     5.8  0  0             0       0     0            0   0   \n",
       "4        36     2.8  0  0             0       0     0            0   0   \n",
       "\n",
       "   020603 ...   zed  zero  zhong  zoe  zogtoriu  zoom  é  ü  üll  〨ud  \n",
       "0       0 ...     0     0      0    0         0     0  0  0    0    0  \n",
       "1       0 ...     0     0      0    0         0     0  0  0    0    0  \n",
       "2       0 ...     0     0      0    0         0     0  0  0    0    0  \n",
       "3       0 ...     0     0      0    0         0     0  0  0    0    0  \n",
       "4       0 ...     0     0      0    0         0     0  0  0    0    0  \n",
       "\n",
       "[5 rows x 7152 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialise CountVectoriser based on the text cleaning function we've just created\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# The document-term matrix will be based on training set only\n",
    "count_vect_fit = count_vect.fit(X_train0['body_text'])\n",
    "# Create the training and test document-term matrices based on counts\n",
    "X_count_train = count_vect_fit.transform(X_train0['body_text'])\n",
    "X_count_test = count_vect_fit.transform(X_test0['body_text'])\n",
    "# Transform them into dataframes\n",
    "X_count_train_df = pd.DataFrame(X_count_train.toarray())\n",
    "X_count_test_df = pd.DataFrame(X_count_test.toarray())\n",
    "# Add column names\n",
    "X_count_train_df.columns = count_vect.get_feature_names()\n",
    "X_count_test_df.columns = count_vect.get_feature_names()\n",
    "# Create the final training and test sets by adding the text length and punct percentage features to the count features\n",
    "X_train = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           X_count_train_df], axis=1)\n",
    "X_test = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           X_count_test_df], axis=1)\n",
    "y_train = y_train0\n",
    "y_test = y_test0\n",
    "# Display some enteries of the training matrix\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4453, 7152)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring parameter settings using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary methods\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\Adnane\\Anaconda3\\envs\\CompletePython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.189965</td>\n",
       "      <td>4.854063</td>\n",
       "      <td>0.271639</td>\n",
       "      <td>0.063503</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.976431</td>\n",
       "      <td>0.977553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.254711</td>\n",
       "      <td>0.222709</td>\n",
       "      <td>0.171984</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.974186</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.978676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.005140</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.775035</td>\n",
       "      <td>0.475367</td>\n",
       "      <td>0.210819</td>\n",
       "      <td>0.048741</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.974186</td>\n",
       "      <td>0.974186</td>\n",
       "      <td>0.978676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972154</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.075604</td>\n",
       "      <td>0.341039</td>\n",
       "      <td>0.326526</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.971942</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.977553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971704</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.998597</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.776272</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>0.160484</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.974186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968561</td>\n",
       "      <td>0.005730</td>\n",
       "      <td>5</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>0.991297</td>\n",
       "      <td>0.993543</td>\n",
       "      <td>0.992705</td>\n",
       "      <td>0.992645</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5      32.189965      4.854063         0.271639        0.063503   \n",
       "2      15.254711      0.222709         0.171984        0.016924   \n",
       "4      16.775035      0.475367         0.210819        0.048741   \n",
       "3      29.075604      0.341039         0.326526        0.069007   \n",
       "0      11.776272      0.496094         0.160484        0.025549   \n",
       "\n",
       "  param_max_depth param_n_estimators  \\\n",
       "5            None                300   \n",
       "2              90                150   \n",
       "4            None                150   \n",
       "3              90                300   \n",
       "0              60                150   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "5  {'max_depth': None, 'n_estimators': 300}           0.973064   \n",
       "2    {'max_depth': 90, 'n_estimators': 150}           0.974186   \n",
       "4  {'max_depth': None, 'n_estimators': 150}           0.974186   \n",
       "3    {'max_depth': 90, 'n_estimators': 300}           0.971942   \n",
       "0    {'max_depth': 60, 'n_estimators': 150}           0.973064   \n",
       "\n",
       "   split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "5           0.976431           0.977553       ...                0.972603   \n",
       "2           0.973064           0.978676       ...                0.972154   \n",
       "4           0.974186           0.978676       ...                0.972154   \n",
       "3           0.975309           0.977553       ...                0.971704   \n",
       "0           0.970819           0.974186       ...                0.968561   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "5        0.004416                1            1.000000            1.000000   \n",
       "2        0.005140                2            0.998877            0.998596   \n",
       "4        0.005240                2            1.000000            1.000000   \n",
       "3        0.004683                4            0.998877            0.998316   \n",
       "0        0.005730                5            0.992981            0.992701   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "5            1.000000            1.000000            1.000000   \n",
       "2            0.999439            0.998596            0.998878   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "3            0.998877            0.998877            0.998597   \n",
       "0            0.991297            0.993543            0.992705   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "5          1.000000         0.000000  \n",
       "2          0.998877         0.000308  \n",
       "4          1.000000         0.000000  \n",
       "3          0.998709         0.000225  \n",
       "0          0.992645         0.000741  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a grid seach with CV using GridSearchCV from sklearn\n",
    "\n",
    "# Initialise the random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "# Parameters to explore\n",
    "param = {'n_estimators': [150, 300],\n",
    "        'max_depth': [60, 90, None]}\n",
    "# Perform the grid search\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_train, y_train0)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation of models\n",
    "\n",
    "Here we will compare the best RF model and the best GBM obtained after tweaking the parameters of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 9.525 / Predict time: 0.198 ---- Precision: 0.119 / Recall: 0.053 / Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test0, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 180.528 / Predict time: 0.13 ---- Precision: 0.092 / Recall: 0.072 / Accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test0, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

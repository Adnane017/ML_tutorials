{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "\n",
    "This notebook reviews the basics of linear algebra, which you will need to understand most of the models that we will cover in our tutorials. We will introduce the elementary concepts (e.g. vectors, matrices, dot product), the mathematical notation that we will use throughout the course, as well as the Python code that represent the different mathematical operations covered using the Numpy package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars\n",
    "\n",
    "These are simply numbers from the set of real numbers $\\mathcal{R}$. \n",
    "We denote a scalar with an ordinary lower-cased letter such as $x$ ($\\in \\mathcal{R}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(3.0)\n",
    "y = np.array(2.0)\n",
    "\n",
    "print('x + y =', x + y) # addition\n",
    "print('x - y =', x - y) # substraction\n",
    "print('x * y =', x * y) # multiplication\n",
    "print('x / y =', x / y) # division\n",
    "print('x ** y =', np.power(x,y)) # exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "You can think of a vector as simply a list of numbers, for example ``[1.0,3.0,4.0,2.0]``.\n",
    "Each of the numbers in the vector consists of a single scalar value. We call these values the *elements* or *components* of the vector. For example, if we are studying the risk that loans default, we might associate each applicant with a vector whose components correspond to their income, length of employment, number of previous defaults, etc. In mathematical notation, we will usually denote vectors as bold-faced, lower-cased letters such as $\\mathbf{u}$.\n",
    "\n",
    "#### Create a vector from a list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u = np.array([1., 3., 4., 2.])\n",
    "print('u =', u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector using numpy.arrange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.arange(4)\n",
    "print('u =', u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access a vector component by index\n",
    "\n",
    "We can refer to any element of a vector by using a subscript.\n",
    "For example, we can refer to the $4$th element of $\\mathbf{u}$ by $u_4$.\n",
    "In code, we access any element $i$ by indexing into the `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic mathematical operations on vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "u = np.array([1, 2, 3])\n",
    "v = np.array([10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('u + v =', u + v)\n",
    "print('u - v =', u - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x * u =', x * u)\n",
    "print('x * u + v =', x * u + v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length of a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Just as vectors generalize scalars from order $0$ to order $1$, matrices generalize vectors from $1D$ to $2D$. Matrices, which we will typically denote with capital letters (e.g. $A$), are represented in code as arrays with 2 axes. Visually, we can draw a matrix as a table, where each entry $a_{ij}$ belongs to the $i$-th row and $j$-th column.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Matrices are useful data structures: they allow us to organize data that has different modalities of variation. For example, rows in our matrix might correspond to different patients, while columns might correspond to different attributes.\n",
    "\n",
    "#### Creating a matrix\n",
    "\n",
    "We can create a matrix with $n$ rows and $m$ columns in Numpy by specifying a shape with two components `(n,m)` when calling any of our favorite functions for instantiating an array such as `arange`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5,4)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing specific elements of a matrix\n",
    "\n",
    "We can access the scalar elements $a_{ij}$ of a matrix $A$ by specifying the indices for the row ($i$) and column ($j$) respectively. Leaving them blank via a `:` takes all elements along the respective dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Element of the 2nd row and 4th column:', A[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1st column:', A[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3rd row:', A[2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transposing a matrix\n",
    "\n",
    "We can transpose the matrix through `T`. That is, if $B = A^T$, then $b_{ij} = a_{ji}$ for any $i$ and $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise multiplication\n",
    "\n",
    "We can perfrorm element-by-element multiplication of two matrices of the same shape using the usual multiplication operator ``*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A*A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size and shape of an array\n",
    "\n",
    "In Numpy, the ``size`` of an array refers to the total number of elements in the array, while the ``shape`` is a tuple that lists the dimensionality along each axis of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('u =', u)\n",
    "print('The size of u: ', u.size)\n",
    "print('The shape of u: ', u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sums and means\n",
    "\n",
    "The next more sophisticated thing we can do with vectors or matrices is to calculate the sum of their elements. In mathematical notation, we express sums using the $\\sum$ symbol.\n",
    "To express the sum of the elements in a vector $\\mathbf{u}$ of length $d$,\n",
    "we can write $\\sum_{i=1}^d u_i$. In code, we can just call ``sum()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(3)\n",
    "print(x)\n",
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly express sums over the elements of matrices of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5,4)\n",
    "print(A)\n",
    "print(A.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity is the *mean*, which is also called the *average*.\n",
    "We calculate the mean by dividing the sum by the total number of elements.\n",
    "With mathematical notation, we could write the average\n",
    "over a vector $\\mathbf{u}$ as $\\frac{1}{d} \\sum_{i=1}^{d} u_i$\n",
    "and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.\n",
    "In code, we could just call ``mean()`` on arrays of arbitrary shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.mean())\n",
    "print(A.sum() / A.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products\n",
    "\n",
    "Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the dot product $\\mathbf{u}^T \\mathbf{v}$ is a sum over the products of the corresponding elements: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_iv_i$. In Numpy, the corresponding function is ``dot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.arange(4)\n",
    "v = np.ones(4)\n",
    "print('u =', u)\n",
    "print('v =', v)\n",
    "print('u.v =', np.dot(u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can express the dot product of two vectors `dot(x, y)` equivalently by performing an element-wise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x * y)\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\mathbf{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\mathbf{u}^T \\mathbf{w}$. When the weights are non-negative and sum to one $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$, the dot product expresses a *weighted average*. When two vectors each have length one (we will discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between them.\n",
    "\n",
    "## Matrix-vector products\n",
    "\n",
    "Now that we know how to calculate dot products we can begin to understand matrix-vector products. Let's start off by visualizing a matrix $A$ and a column vector $\\mathbf{u}$.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{u}=\\begin{pmatrix}\n",
    " u_{1}  \\\\\n",
    " u_{2} \\\\\n",
    "\\vdots\\\\\n",
    " u_{m}\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We can visualize the matrix in terms of its row vectors\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^T_{i} \\in \\mathbb{R}^{m}$\n",
    "is a row vector representing the $i$-th row of the matrix $A$.\n",
    "\n",
    "Then the matrix vector product $\\mathbf{v} = A\\mathbf{u}$ is simply a column vector $\\mathbf{v} \\in \\mathbb{R}^n$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\mathbf{u}$.\n",
    "\n",
    "$$A\\mathbf{u}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1}  \\\\\n",
    "\\mathbf{a}^T_{2}  \\\\\n",
    " \\vdots  \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " u_{1}  \\\\\n",
    " u_{2} \\\\\n",
    "\\vdots\\\\\n",
    " u_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{u}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{u} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{u}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So you can think of multiplication by a matrix $A\\in \\mathbb{R}^{n \\times m}$ as a transformation that projects vectors from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$.\n",
    "\n",
    "These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. We can also use matrix-vector products to describe the calculations of each layer in a neural network.\n",
    "\n",
    "To express matrix-vector products in code, we use the same ``dot()`` function as for dot products. When we call ``np.dot(A, u)`` with a matrix ``A`` and a vector ``u``, Numpy knows to perform a matrix-vector product. Note that the column dimension of ``A`` must be the same as the dimension of ``u``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.dot(A, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-matrix multiplication\n",
    "\n",
    "Say we have two matrices, $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "B=\\begin{pmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad B=\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathbb{R}^k$ and that each column vector $\\mathbf{b}_j$ also lies in $\\mathbb{R}^k$.\n",
    "\n",
    "Then to produce the matrix product $C \\in \\mathbb{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\mathbf{b}_j$.\n",
    "\n",
    "$$C = AB = \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix. Just as with ordinary dot products and matrix-vector products, we can compute matrix-matrix products in Numpy by using ``dot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5,4)\n",
    "B = np.ones(shape=(4, 3))\n",
    "print('A =\\n', A)\n",
    "print('B =\\n', B)\n",
    "print('AB =\\n', np.dot(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting a matrix\n",
    "\n",
    "#### Identity matrix\n",
    "\n",
    "One special matrix that we will need to cover before introducing matrix inverses is the identity matrix. This is a squared matrix (i.e. has the same number of rows and columns) whose elements are $0$, except the diagonal elements, which are equal to $1$. We often denote an identity matrix by $I_n \\in \\mathbb{R}^{n \\times n}$, where $n$ is the dimension of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(3)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse of a  matrix\n",
    "\n",
    "The matrix inverse of $A$ is denoted as $A^{−1}$, and it is deﬁned as the matrix such that $A^{−1}A = I_n$. In code, the inverse can be computed using ``linalg.inv()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = np.array([[2,3],[6,1]])\n",
    "print('M =\\n', M)\n",
    "print('inv(M) =\\n', np.linalg.inv(M))\n",
    "print('M*inv(M) =\\n', np.dot(M, np.linalg.inv(M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinants\n",
    "\n",
    "The inverse of a matrix does not always exists. To check whether we can inverse a matrix $A$, we can calculate its determinant, which is denoted by $det(A)$, and calculated in Numpy using ``linalg.det()``. The inverse exists if and only if $det(A)\\neq0$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M = np.array([[2,3],[6,1]])\n",
    "print('M =\\n', M)\n",
    "print('det(M) =\\n', np.linalg.det(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear systems of equations\n",
    "\n",
    "Matrix multiplication in matrix inverses are particularly useful concepts for solving linear systems of equation. They simplify notation and offer a structured way to solve such systems. For example, a linear system of equations of the form:\n",
    "\n",
    "$$\\begin{array}{ccl} \n",
    "a_{11}x_1 + a_{11}x_2 + \\cdots + a_{1n}x_n & = & b_1\\\\ \n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n & = & b_2\\\\\n",
    "\\vdots    & = & \\vdots\\\\\n",
    "a_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n & = & b_n\n",
    "\\end{array}$$\n",
    "\n",
    "can be re-written simply as: $Ax = b$, where:\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nn} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "\\mathbf{b}=\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\\\\\n",
    "\\end{pmatrix},\\quad\n",
    "\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "so if A can be inverted, the unknow vector $\\mathbf{x}$ can be computed as follows: $\\mathbf{x} =A^{-1}\\mathbf{b}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "Some of the most useful operators in linear algebra are norms.\n",
    "Informally, they tell us how big a vector or matrix is.\n",
    "We represent norms with the notation $\\|\\cdot\\|$.\n",
    "The $\\cdot$ in this expression is just a placeholder.\n",
    "For example, we would represent the norm of a vector $\\mathbf{u}$\n",
    "or matrix $A$ as $\\|\\mathbf{u}\\|$ or $\\|A\\|$, respectively.\n",
    "\n",
    "All norms must satisfy four properties:\n",
    "\n",
    "1. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
    "1. $\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n",
    "1. $\\|A\\| \\geq 0$\n",
    "1. If $\\|A\\|=0$, then $\\forall {i,j}, a_{ij} = 0$\n",
    "\n",
    "To put it in words, the first rule says\n",
    "that if we scale all the components of a matrix or vector\n",
    "by a constant factor $\\alpha$,\n",
    "its norm also scales by the *absolute value*\n",
    "of the same constant factor.\n",
    "The second rule is the familiar triangle inequality.\n",
    "The third rule simply says that the norm must be non-negative.\n",
    "The final rule basically says that the smallest norm is achieved by a matrix or vector consisting of all zeros.\n",
    "\n",
    "More often, in machine learning we work with the $\\ell_2$ norm, which is the sum of the squared components. \n",
    "We also commonly work with the $\\ell_1$ norm.\n",
    "The $\\ell_1$ norm is simply the sum of the absolute values.\n",
    "It has the convenient property of placing less emphasis on outliers.\n",
    "\n",
    "To calculate the $\\ell_2$ norm, we can just call ``norm()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('u =', u)\n",
    "print('norm of u:', np.linalg.norm(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the $\\ell_1$ norm we can simply perform the absolute value and then sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances\n",
    "\n",
    "A distance between two vectors tells us how far they are from each other. If we have two vectors $u$\n",
    "and $v$, their distance is denoted as $d(u,v)$. A distance should verify four conditions:\n",
    "\n",
    "1. $d(u,v) \\geq 0$\n",
    "1. If $d(u,v)=0$, then $u=v$\n",
    "1. $d(u,v)=d(v,u)$\n",
    "1. $d(u,v) \\leq d(u,w) + d(w,v)$\n",
    "\n",
    "One of the most used distance measures is the Euclidean distance, which is defined as folllows: If $u = (u_1, u_2, \\cdots, u_n)$ and $v = (v_1, v_2, \\cdots, v_n)$ are two vectors, then $d(u,v)=\\sqrt{(u_1-v_1)^2 + \\cdots + (u_n-v_n)^2}$. The Euclidean distance is also known as the $\\ell_2$-norm distance. As with norms, there is also the $\\ell_1$-norm distance, which is very similar to the $\\ell_2$ one, with the difference being that we use absolute values instead of the exponents $2$, that is: $d(u,v)=\\sqrt{|u_1-v_1| + \\cdots + |u_n-v_n|}$. \n",
    "\n",
    "In Numpy, there is no direct function to compute distances, but we can use the fact that, for the $\\ell_r$-norm distances, $d(u,v) = \\|u-v\\|$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.arange(4)\n",
    "v = np.ones(4)\n",
    "print('u =', u)\n",
    "print('v =', v)\n",
    "print('d(u,v) =', np.linalg.norm(u-v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms and distances are widely used concepts in machine learning because we often try to solve optimization problems such as *minimising* the distance between predictions and the ground-truth observations or assigning vector representations to items (like words, products, or news articles) so that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, these objectives, perhaps the most important component of a machine learning algorithm (besides the data itself), are expressed as norms.\n",
    "\n",
    "\n",
    "## Intermediate linear algebra\n",
    "\n",
    "You already know nearly all of the linear algebra required\n",
    "to implement many practically useful models. But there is a lot more to linear algebra, even as concerns machine learning. In this section, we introduce some useful, more advanced concepts.\n",
    "\n",
    "#### Special matrices\n",
    "\n",
    "There are a number of special matrices that we will use throughout this tutorial. Let's look at them in a bit of detail:\n",
    "\n",
    "* **Symmetric Matrix** These are matrices where the entries below and above the diagonal are the same. In other words, we have that $M^\\top = M$. An example of such matrices are those that describe pairwise distances, i.e. $M_{ij} = \\|x_i - x_j\\|$. Likewise, the Facebook friendship graph can be written as a symmetric matrix where $M_{ij} = 1$ if $i$ and $j$ are friends and $M_{ij} = 0$ if they are not. Note that the *Twitter* graph is asymmetric - $M_{ij} = 1$, i.e. $i$ following $j$ does not imply that $M_{ji} = 1$, i.e. $j$ following $i$.\n",
    "* **Antisymmetric Matrix** These matrices satisfy $M^\\top = -M$. Note that any square matrix can always be decomposed into a symmetric and into an antisymmetric matrix by using $M = \\frac{1}{2}(M + M^\\top) + \\frac{1}{2}(M - M^\\top)$.\n",
    "* **Positive Definite Matrix** These are matrices that have the nice property where $x^\\top M x > 0$ whenever $x \\neq 0$. Intuitively, they are a generalization of the squared norm of a vector $\\|x\\|^2 = x^\\top x$. It is easy to check that whenever $M = A^\\top A$, this holds since there $x^\\top M x = x^\\top A^\\top A x = \\|A x\\|^2$. There is a somewhat more profound theorem which states that all positive definite matrices can be written in this form.\n",
    "\n",
    "#### Basic vector properties\n",
    "\n",
    "Vectors are useful beyond being data structures to carry numbers.\n",
    "In addition to reading and writing values to the components of a vector,\n",
    "and performing some useful mathematical operations,\n",
    "we can analyze vectors in some interesting ways.\n",
    "\n",
    "One important concept is the notion of a vector space.\n",
    "Here are the conditions that make a vector space:\n",
    "\n",
    "* **Additive axioms** (we assume that x,y,z are all vectors):\n",
    "  $x+y = y+x$ and $(x+y)+z = x+(y+z)$ and $0+x = x+0 = x$ and $(-x) + x = x + (-x) = 0$.\n",
    "* **Multiplicative axioms** (we assume that x is a vector and a, b are scalars):\n",
    "  $0 \\cdot x = 0$ and $1 \\cdot x = x$ and $(a b) x = a (b x)$.\n",
    "* **Distributive axioms** (we assume that x and y are vectors and a, b are scalars):\n",
    "  $a(x+y) = ax + ay$ and $(a+b)x = ax +bx$.\n",
    "\n",
    "#### Ressources \n",
    "\n",
    "If you are eager to learn more about linear algebra, here are some very interesting resources on the topic\n",
    "\n",
    "* Grant Sanderson's Youtube course [Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B)\n",
    "* Zico Kolter's [Linear Algebra Review and Reference](http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
